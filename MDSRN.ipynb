{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65132e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from torch import optim\n",
    "\n",
    "class BatchIndex:\n",
    "    def __init__(self, size, batch_size, shuffle=True):\n",
    "        self.index_list = torch.as_tensor([(x, min(x + batch_size, size)) for x in range(0, size, batch_size)])\n",
    "        \n",
    "        if shuffle:\n",
    "            self.index_list = self.index_list[torch.randperm(len(self.index_list))]\n",
    "        \n",
    "        self.pos = -1\n",
    "\n",
    "    def __next__(self):\n",
    "        self.pos += 1\n",
    "        if self.pos >= len(self.index_list):\n",
    "            raise StopIteration\n",
    "        return self.index_list[self.pos]\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.pos = -1\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_list)\n",
    "    \n",
    "def get_mgrid(sidelen, dim=2, s=1,t=0):\n",
    "    '''Generates a flattened grid of (x,y,...) coordinates in a range of -1 to 1.'''\n",
    "    if isinstance(sidelen, int):\n",
    "        sidelen = dim * (sidelen,)\n",
    "\n",
    "    if dim == 2:\n",
    "        pixel_coords = np.stack(np.mgrid[:sidelen[0]:s, :sidelen[1]:s], axis=-1)[None, ...].astype(np.float32)\n",
    "        pixel_coords[..., 0] = pixel_coords[..., 0] / (sidelen[0] - 1)\n",
    "        pixel_coords[..., 1] = pixel_coords[..., 1] / (sidelen[1] - 1)\n",
    "    elif dim == 3:\n",
    "        ranges = [\n",
    "            torch.arange(0, sidelen[0], s, device='cuda:0'),\n",
    "            torch.arange(0, sidelen[1], s, device='cuda:0'),\n",
    "            torch.arange(0, sidelen[2], s, device='cuda:0'),\n",
    "        ]\n",
    "        grid = torch.meshgrid(ranges, indexing='ij')\n",
    "        pixel_coords = torch.stack(grid, dim=-1)[None, ...].float()\n",
    "        pixel_coords[..., 0] = pixel_coords[..., 0] / (sidelen[0] - 1)\n",
    "        pixel_coords[..., 1] = pixel_coords[..., 1] / (sidelen[1] - 1)\n",
    "        pixel_coords[..., 2] = pixel_coords[..., 2] / (sidelen[2] - 1)\n",
    "    elif dim == 4:\n",
    "        pixel_coords = np.stack(np.mgrid[:sidelen[0]:(t+1), :sidelen[1]:s, :sidelen[2]:s, :sidelen[3]:s], axis=-1)[None, ...].astype(np.float32)\n",
    "        pixel_coords[..., 0] = pixel_coords[..., 0] / max(sidelen[0] - 1, 1)\n",
    "        pixel_coords[..., 1] = pixel_coords[..., 1] / (sidelen[1] - 1)\n",
    "        pixel_coords[..., 2] = pixel_coords[..., 2] / (sidelen[2] - 1)\n",
    "        pixel_coords[..., 3] = pixel_coords[..., 3] / (sidelen[3] - 1)\n",
    "    else:\n",
    "        raise NotImplementedError('Not implemented for dim=%d' % dim)\n",
    "    pixel_coords = 2. * pixel_coords - 1.\n",
    "    pixel_coords = pixel_coords.cpu().numpy().reshape(-1,3, order='F')\n",
    "    return pixel_coords\n",
    "\n",
    "def fast_random_choice(dim, num_samples_per_frame, unique=True, device='cuda:0'):\n",
    "    if unique:\n",
    "        num_samples = num_samples_per_frame * 2  # 防止去重后低于预定采样值\n",
    "        x = torch.randint(\n",
    "                0, dim[0], size=(num_samples,), device='cuda:0'\n",
    "            )\n",
    "        y = torch.randint(\n",
    "                0, dim[1], size=(num_samples,), device='cuda:0'\n",
    "            )\n",
    "        z = torch.randint(\n",
    "                0, dim[2], size=(num_samples,), device='cuda:0'\n",
    "            )\n",
    "        \n",
    "        xyz = torch.stack([x, y, z], dim=-1)\n",
    "        _, index = torch.unique(xyz, dim=0, sorted=False, return_inverse=True)\n",
    "        xyz = xyz[index[:num_samples_per_frame, ...]]\n",
    "        return xyz[...,0], xyz[...,1], xyz[...,2]\n",
    "    else:\n",
    "        x = torch.randint(\n",
    "                0, dim[0], size=(num_samples_per_frame,), device='cuda:0'\n",
    "            )\n",
    "        y = torch.randint(\n",
    "                0, dim[1], size=(num_samples_per_frame,), device='cuda:0'\n",
    "            )\n",
    "        z = torch.randint(\n",
    "                0, dim[2], size=(num_samples_per_frame,), device='cuda:0'\n",
    "            )\n",
    "        xyz = torch.stack([x, y, z], dim=-1)\n",
    "        if device == 'cpu':\n",
    "            xyz = xyz.cpu()\n",
    "        return xyz[...,0], xyz[...,1], xyz[...,2]\n",
    "    \n",
    "def count_params(model):  # 查看模型参数量\n",
    "    param_num = sum(p.numel() for p in model.parameters())\n",
    "    return param_num\n",
    "\n",
    "def cleanup():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bad143eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import threading\n",
    "import queue\n",
    "import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "class ScalarDataSet():\n",
    "    def __init__(self,args, device='cuda:0'):\n",
    "        self.dataset = args.dataset\n",
    "        self.batch_size = args.batch_size\n",
    "        self.interval = args.interval\n",
    "        self.downsample_factor = args.downsample_factor\n",
    "        self.device = device\n",
    "\n",
    "        if self.dataset == 'h2':\n",
    "            self.dim = [600, 248, 248]\n",
    "            self.total_samples = 1\n",
    "            self.data_path = './dataset/h2/'        \n",
    "        elif self.dataset == 'fivejets':\n",
    "            self.dim = [128, 128, 128]\n",
    "            self.total_samples = 1\n",
    "            self.data_path = './dataset/fivejets/' \n",
    "        elif self.dataset == 'combustion':\n",
    "            self.dim = [480, 720, 120]\n",
    "            self.total_samples = 1\n",
    "            self.data_path = './dataset/combustion/'\n",
    "        elif self.dataset == 'halfcy':\n",
    "            self.dim = [640, 240, 80]\n",
    "            self.total_samples = 1\n",
    "            self.data_path = './dataset/halfcy/'\n",
    "        elif self.dataset == 'tornado':\n",
    "            self.dim = [128, 128, 128]\n",
    "            self.total_samples = 1\n",
    "            self.data_path = './dataset/tornado/'\n",
    "        elif self.dataset == 'vortex':\n",
    "            self.dim = [128, 128, 128]\n",
    "            self.total_samples = 1\n",
    "            self.data_path = './dataset/vortex/'\n",
    "\n",
    "        self.num_workers = 16\n",
    "\n",
    "        self.samples = [i for i in range(1,self.total_samples+1,self.interval+1)]\n",
    "        self.total_samples = self.samples[-1]\n",
    "        self.num_samples_per_frame = (self.dim[0]*self.dim[1]*self.dim[2]//self.downsample_factor)//self.batch_size * self.batch_size\n",
    "        # self.num_samples_per_frame = 4 * self.batch_size\n",
    "        # self.num_samples_per_frame = self.dim[0]*self.dim[1]*self.dim[2]\n",
    "\n",
    "        self.queue_size = 2\n",
    "        self.loader_queue = queue.Queue(maxsize=self.queue_size)  # 限制队列大小为2\n",
    "        self.executor = ThreadPoolExecutor(max_workers=self.queue_size)\n",
    "\n",
    "        if args.mode == 'train':\n",
    "            self.data = self.preload_with_multi_threads(self.load_volume_data, num_workers=self.num_workers, data_str='Volume Data')\n",
    "            self.data = torch.as_tensor(np.asarray(self.data), device=self.device)  # [t个时间步, z, y, x] 需要改成xyz的形式\n",
    "\n",
    "            self.len = self.num_samples_per_frame * len(self.samples)\n",
    "            self._get_data = self._get_training_data\n",
    "\n",
    "            samples = self.dim[2]*self.dim[1]*self.dim[0]\n",
    "            self.coords = get_mgrid([self.dim[0],self.dim[1],self.dim[2]],dim=3)\n",
    "            self.time = np.zeros((samples,1))\n",
    "            self.testing_data_inputs = torch.as_tensor(np.concatenate((self.time, self.coords),axis=1), dtype=torch.float, device='cuda:0')\n",
    "            self.preload_data()\n",
    "\n",
    "        elif args.mode == 'inf':\n",
    "            samples = self.dim[2]*self.dim[1]*self.dim[0]\n",
    "            self.coords = get_mgrid([self.dim[0],self.dim[1],self.dim[2]],dim=3)\n",
    "            self.time = np.zeros((samples,1))\n",
    "            self.testing_data_inputs = torch.as_tensor(np.concatenate((self.time, self.coords),axis=1), dtype=torch.float, device='cuda:0')\n",
    "            # self._get_data = self._get_testing_data\n",
    "            # self.preload_data()\n",
    "\n",
    "    def preload_data(self):\n",
    "        if self.loader_queue.full():\n",
    "            return  # 如果队列已满，不进行加载\n",
    "        self.loader_queue.put(self._get_data())\n",
    "\n",
    "    def get_data(self):\n",
    "        if self.loader_queue.empty():\n",
    "            print(\"DataLoader is not ready yet! Waiting...\")\n",
    "        while self.loader_queue.empty():\n",
    "            pass\n",
    "        # 获取当前 DataLoader 并异步加载下一个\n",
    "        current_data = self.loader_queue.get()\n",
    "        self.executor.submit(self.preload_data)\n",
    "        return current_data\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _get_testing_data(self, idx):\n",
    "        t = idx - 1\n",
    "        t = t / max((self.total_samples-1), 1)\n",
    "        t = 2.0 * t - 1.0\n",
    "        testing_data_inputs = self.testing_data_inputs.clone()\n",
    "        testing_data_inputs[:,0] = t\n",
    "        batchidxgenerator = BatchIndex(testing_data_inputs.shape[0], self.batch_size, False)\n",
    "        return testing_data_inputs, batchidxgenerator\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _get_training_data(self):\n",
    "        training_data_inputs = []\n",
    "        training_data_outputs = []\n",
    "\n",
    "        for i in range(0, len(self.samples)):\n",
    "            x,y,z = fast_random_choice(self.dim, self.num_samples_per_frame)\n",
    "            t = torch.ones_like(x) * (self.samples[i]-1)\n",
    "\n",
    "            outputs = self.data[i, x, y, z]  # 第i个体数据中取xyz, 第i个体数据对应的时间步是t\n",
    "            # 归一化\n",
    "            x = x / (self.dim[0] - 1)\n",
    "            y = y / (self.dim[1] - 1)\n",
    "            z = z / (self.dim[2] - 1)\n",
    "            t = t / max((self.total_samples-1), 1)\n",
    "\n",
    "            inputs = torch.stack([t, x, y, z], dim=-1)\n",
    "            inputs = 2.0 * inputs - 1.0  # 缩放到[-1,1]\n",
    "            training_data_inputs.append(inputs)\n",
    "            training_data_outputs.append(outputs)\n",
    "\n",
    "        training_data_inputs = torch.cat(training_data_inputs, dim=0).cuda()\n",
    "        training_data_outputs = torch.cat(training_data_outputs, dim=0).cuda()\n",
    "        idx = torch.randperm(training_data_inputs.shape[0], device='cpu')\n",
    "        training_data_inputs = training_data_inputs[idx].contiguous()\n",
    "        training_data_outputs = training_data_outputs[idx].contiguous()\n",
    "        batchidxgenerator = BatchIndex(self.len, self.batch_size, shuffle=True)\n",
    "        del idx\n",
    "        cleanup()\n",
    "        return training_data_inputs, training_data_outputs, batchidxgenerator\n",
    "\n",
    "    def load_volume_data(self, idx):\n",
    "        d = np.fromfile(self.data_path+'{:04d}.raw'.format(self.samples[idx]), dtype='<f')\n",
    "        d = 2. * (d - np.min(d)) / (np.max(d) - np.min(d)) - 1.  # FIXME: 每一帧范围都不一样，有助于时间超分？\n",
    "        # d = 2. * (d - self.data_min) / (self.data_max - self.data_min) - 1.\n",
    "        d = d.reshape(self.dim[2],self.dim[1],self.dim[0])  # 以x变化最大的形式存放的，读取时需要倒过来读\n",
    "        d = d.transpose(2,1,0)  # 转化成xyz三维数组形式\n",
    "        return d\n",
    "\n",
    "    def _preload_worker(self, data_list, load_func, q, lock, idx_tqdm):\n",
    "        # Keep preloading data in parallel.\n",
    "        while True:\n",
    "            idx = q.get()\n",
    "            data_list[idx] = load_func(idx)\n",
    "            with lock:\n",
    "                idx_tqdm.update()\n",
    "            q.task_done()\n",
    "\n",
    "    def preload_with_multi_threads(self, load_func, num_workers, data_str='images'):\n",
    "        data_list = [None] * len(self.samples)\n",
    "\n",
    "        q = queue.Queue(maxsize=len(self.samples))\n",
    "        idx_tqdm = tqdm.tqdm(range(len(self.samples)), desc=f\"Loading {data_str}\", leave=False)\n",
    "        for i in range(len(self.samples)):\n",
    "            q.put(i)\n",
    "        lock = threading.Lock()\n",
    "        for ti in range(num_workers):\n",
    "            t = threading.Thread(target=self._preload_worker,\n",
    "                                    args=(data_list, load_func, q, lock, idx_tqdm), daemon=True)\n",
    "            t.start()\n",
    "        q.join()\n",
    "        idx_tqdm.close()\n",
    "        assert all(map(lambda x: x is not None, data_list))\n",
    "\n",
    "        return data_list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df180441",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class SineLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True,\n",
    "                 is_first=False, omega_0=30):\n",
    "        super().__init__()\n",
    "        self.omega_0 = omega_0\n",
    "        self.is_first = is_first\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        with torch.no_grad():\n",
    "            if self.is_first:\n",
    "                self.linear.weight.uniform_(-1 / self.in_features, \n",
    "                                             1 / self.in_features)      \n",
    "            else:\n",
    "                self.linear.weight.uniform_(-np.sqrt(6 / self.in_features) / self.omega_0, \n",
    "                                             np.sqrt(6 / self.in_features) / self.omega_0)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return torch.sin(self.omega_0 * self.linear(input))\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self,in_features,out_features,nonlinearity='relu'):\n",
    "        super(ResBlock,self).__init__()\n",
    "\n",
    "        self.net = []\n",
    "\n",
    "        self.net.append(SineLayer(in_features,out_features))\n",
    "\n",
    "        self.net.append(SineLayer(out_features,out_features))\n",
    "\n",
    "        self.flag = (in_features!=out_features)\n",
    "\n",
    "        if self.flag:\n",
    "            self.transform = SineLayer(in_features,out_features)\n",
    "\n",
    "        self.net = nn.Sequential(*self.net)\n",
    "    \n",
    "    def forward(self,features):\n",
    "        outputs = self.net(features)\n",
    "        if self.flag:\n",
    "            features = self.transform(features)\n",
    "        return 0.5*(outputs+features)\n",
    "\n",
    "class CoordNet(nn.Module):\n",
    "    def __init__(self, in_features, out_features, init_features=64,num_res = 10):\n",
    "        super(CoordNet,self).__init__()\n",
    "\n",
    "        self.num_res = num_res\n",
    "\n",
    "        self.net = []\n",
    "\n",
    "        self.net.append(ResBlock(in_features,init_features))\n",
    "        self.net.append(ResBlock(init_features,2*init_features))\n",
    "        self.net.append(ResBlock(2*init_features,4*init_features))\n",
    "\n",
    "        for i in range(self.num_res):\n",
    "            self.net.append(ResBlock(4*init_features,4*init_features))\n",
    "        self.net = nn.Sequential(*self.net)\n",
    "        \n",
    "        self.fc1 = ResBlock(4*init_features, out_features)\n",
    "        self.fc2 = ResBlock(4*init_features, out_features)\n",
    "        self.fc3 = ResBlock(4*init_features, out_features)\n",
    "\n",
    "        self.n_output_dims = out_features\n",
    "\n",
    "    def forward(self, coords):\n",
    "        output = self.net(coords)\n",
    "        out1 = self.fc1(output)\n",
    "        out2 = self.fc2(output)\n",
    "        out3 = self.fc3(output)\n",
    "        out = (out1+out2+out3)/3\n",
    "        data = torch.stack([out1,out2,out3], axis=-1)\n",
    "        var = torch.var(data, axis=-1)\n",
    "        return out, var, out1, out2, out3\n",
    "    \n",
    "class LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        with torch.no_grad():\n",
    "            self.linear.weight.uniform_(-1 / self.in_features, 1 / self.in_features) \n",
    "            #self.linear.weight.normal_(0,0.05) \n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.linear(input)\n",
    "\n",
    "class BottleNeckBlock(nn.Module):\n",
    "    def __init__(self,in_features):\n",
    "        super(BottleNeckBlock,self).__init__()\n",
    "        self.net = []\n",
    "\n",
    "        self.net.append(SineLayer(in_features, in_features//4))\n",
    "\n",
    "        self.net.append(SineLayer(in_features//4, in_features//4))\n",
    "\n",
    "        self.net.append(SineLayer(in_features//4, in_features))\n",
    "\n",
    "        self.net = nn.Sequential(*self.net)\n",
    "    \n",
    "    def forward(self, features):\n",
    "        outputs = self.net(features)\n",
    "        return outputs+features\n",
    "    \n",
    "class CoordNetBottleNeck(nn.Module):\n",
    "    def __init__(self, in_features, out_features, init_features=64,num_res = 10):\n",
    "        super(CoordNetBottleNeck,self).__init__()\n",
    "\n",
    "        self.num_res = num_res\n",
    "\n",
    "        self.net = []\n",
    "        self.net.append(SineLayer(in_features,init_features))\n",
    "        self.net.append(SineLayer(init_features,2*init_features))\n",
    "        self.net.append(SineLayer(2*init_features,4*init_features))\n",
    "\n",
    "        for i in range(self.num_res):\n",
    "            self.net.append(BottleNeckBlock(4*init_features))\n",
    "        self.net = nn.Sequential(*self.net)\n",
    "        \n",
    "        self.fc1 = ResBlock(4*init_features, out_features)\n",
    "        self.fc2 = ResBlock(4*init_features, out_features)\n",
    "        self.fc3 = ResBlock(4*init_features, out_features)\n",
    "\n",
    "    def forward(self, coords):\n",
    "        output = self.net(coords)\n",
    "        out1 = self.fc1(output)\n",
    "        out2 = self.fc2(output)\n",
    "        out3 = self.fc3(output)\n",
    "        out = (out1+out2+out3)/3\n",
    "        data = torch.stack([out1,out2,out3], axis=-1)\n",
    "        var = torch.var(data, axis=-1)\n",
    "        return out, var, out1, out2, out3\n",
    "    \n",
    "    \n",
    "import torch.nn.functional as F\n",
    "class FreqEmbedder:\n",
    "    def __init__(self, multi_freq, include_input=True, input_dims=3, log_sampling=True):\n",
    "        self.multi_freq = multi_freq\n",
    "        self.input_dims = input_dims\n",
    "        self.include_input = include_input\n",
    "        self.log_sampling = log_sampling\n",
    "        self.periodic_fns = [torch.sin, torch.cos]\n",
    "\n",
    "        self.embed_fns = None\n",
    "        self.out_dim = None\n",
    "        self.create_embedding_fn()\n",
    "\n",
    "    def create_embedding_fn(self):\n",
    "        embed_fns = []\n",
    "        d = self.input_dims\n",
    "        out_dim = 0\n",
    "        if self.include_input:\n",
    "            embed_fns.append(lambda x: x)\n",
    "            out_dim += d\n",
    "\n",
    "        max_freq = self.multi_freq - 1\n",
    "        N_freqs = self.multi_freq\n",
    "\n",
    "        if self.log_sampling:\n",
    "            freq_bands = 2. ** torch.linspace(0., max_freq, steps=N_freqs)\n",
    "        else:\n",
    "            freq_bands = torch.linspace(2. ** 0., 2. ** max_freq, steps=N_freqs)\n",
    "\n",
    "        for freq in freq_bands:\n",
    "            for p_fn in self.periodic_fns:\n",
    "                embed_fns.append(lambda x, p_fn=p_fn, freq=freq: p_fn(x * freq))\n",
    "                out_dim += d\n",
    "\n",
    "        self.embed_fns = embed_fns\n",
    "        self.out_dim = out_dim\n",
    "    def embed(self, inputs):\n",
    "        return torch.cat([fn(inputs) for fn in self.embed_fns], -1)\n",
    "\n",
    "class FCLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.relu(self.linear(input))\n",
    "    \n",
    "class Nerf(nn.Module):\n",
    "    def __init__(self, input_dim=4, mi_dim=256, output_dim=1, fourier_dim=84):\n",
    "        super(Nerf, self).__init__()\n",
    "        # 傅里叶编码层\n",
    "        self.fourier_encoding = FreqEmbedder(multi_freq=10, include_input=True, input_dims=3, log_sampling=True)\n",
    "        \n",
    "        # 全连接层\n",
    "        self.net = []\n",
    "        self.net.append(FCLayer(84,mi_dim))\n",
    "        for i in range(8):\n",
    "            self.net.append(FCLayer(mi_dim,mi_dim)) \n",
    "        self.net = nn.Sequential(*self.net)\n",
    "        \n",
    "        self.out1 = nn.Linear(256, output_dim)\n",
    "        self.out2 = nn.Linear(256, output_dim)\n",
    "        self.out3 = nn.Linear(256, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 傅里叶编码\n",
    "        x = self.fourier_encoding.embed(x)\n",
    "        x = self.net(x)\n",
    "\n",
    "        out1 = self.out1(x)\n",
    "        out2 = self.out2(x)\n",
    "        out3 = self.out3(x)\n",
    "        out = (out1+out2+out3)/3\n",
    "        data = torch.stack([out1,out2,out3], axis=-1)\n",
    "        var = torch.var(data, axis=-1)\n",
    "        return out, var, out1, out2, out3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f11e3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "from datetime import datetime\n",
    "from shutil import copy, copytree\n",
    "import json\n",
    "import time\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import math\n",
    "\n",
    "def kl_d(var, mse):\n",
    "    mse = mse/mse.sum().detach()\n",
    "    var = var/var.sum()\n",
    "    kl_loss = torch.nn.functional.kl_div(\n",
    "        torch.log(var+1.e-16),\n",
    "        torch.log(mse+1.e-16),\n",
    "        reduction='none',\n",
    "        log_target=True,\n",
    "    ).mean()\n",
    "    return kl_loss\n",
    "\n",
    "def trainNet(model,args,dataset):\n",
    "    result_dir = os.path.join(args.result_dir, f'{args.dataset}', f'MDSRN')\n",
    "\n",
    "    logs_dir = os.path.join(result_dir, 'logs')\n",
    "    checkpoints_dir = os.path.join(result_dir, 'checkpoints')\n",
    "    outputs_dir = os.path.join(result_dir, 'outputs')\n",
    "    os.makedirs(logs_dir, exist_ok=True)\n",
    "    os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "    os.makedirs(outputs_dir, exist_ok=True)\n",
    "    \n",
    "    loss_log_file = result_dir+'/'+'loss-'+'-'+str(args.interval)+'-'+str(args.init)+'-'+str(args.active)+'.txt'\n",
    "    # todo: FusedAdam训练很不稳定\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr, betas=(0.9,0.999), weight_decay=1e-6, fused=True)\n",
    "    # optimizer = FusedAdam(model.parameters(), lr=args.lr, betas=(0.9,0.999), weight_decay=1e-2)\n",
    "    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.1)\n",
    "    mse_loss = nn.MSELoss()\n",
    "    # 初始化梯度缩放器\n",
    "    scaler = GradScaler(enabled=args.fp16)\n",
    "    \n",
    "    t = 0\n",
    "    start_time = time.time()\n",
    "    with open(loss_log_file,\"a\") as f:\n",
    "        f.write(f\"time:{time.time()}\")\n",
    "        f.write('\\n')\n",
    "    for epoch in range(1,args.num_epochs+1):\n",
    "        model.train()\n",
    "        training_data_inputs, training_data_outputs, batchIndexGenerator = dataset.get_data()\n",
    "        loss_mse = 0\n",
    "        loss_kl = 0\n",
    "        loss_grad = 0\n",
    "        loop = tqdm.tqdm(batchIndexGenerator)\n",
    "        l = 30*(500**((epoch-1) / (args.num_epochs-1))-1)/499 \n",
    "\n",
    "        for current_idx, next_idx in loop:\n",
    "            coord = training_data_inputs[current_idx:next_idx].contiguous()\n",
    "            v = training_data_outputs[current_idx:next_idx].contiguous()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            with autocast(enabled=args.fp16):\n",
    "                mean, var, out1, out2, out3 = model(coord)\n",
    "                #mse = mse_loss(mean.view(-1),v.view(-1))\n",
    "                mse = (mse_loss(out1.view(-1),v.view(-1)) + mse_loss(out2.view(-1),v.view(-1)) + mse_loss(out3.view(-1),v.view(-1))) / 3\n",
    "                kl = kl_d(var.view(-1), ((mean.view(-1)-v.view(-1))**2))\n",
    "                loss = mse + l*kl\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            loss_mse += mse.mean().item()\n",
    "            loss_kl += kl.mean().item()\n",
    "\n",
    "            loop.set_description(f'Epoch [{epoch}/{args.num_epochs}]')\n",
    "            loop.set_postfix(mse=loss_mse, kl=loss_kl)\n",
    "        adjust_lr(args, optimizer, epoch)\n",
    "        # scheduler.step()\n",
    "\n",
    "        with open(loss_log_file,\"a\") as f:\n",
    "            f.write(f\"Epochs {epoch}: loss = {loss_mse}, lr = {optimizer.param_groups[0]['lr']}\")\n",
    "            f.write('\\n')\n",
    "\n",
    "        if epoch%args.checkpoint == 0 or epoch==1:\n",
    "            torch.save(model.state_dict(),checkpoints_dir+'/'+'-'+str(args.interval)+'-'+str(args.init)+'-'+str(epoch)+'.pth')\n",
    "            \n",
    "    with open(loss_log_file,\"a\") as f:\n",
    "        f.write(f\"time:{time.time()}\")\n",
    "        f.write(f\"time:{time.time()-start_time}\")\n",
    "        f.write('\\n')\n",
    "\n",
    "@torch.no_grad()\n",
    "def inf(model,dataset,args, result_dir=None):\n",
    "    ckpt = './'+args.dataset+args.ckpt\n",
    "    result_dir = os.path.dirname(os.path.dirname(ckpt)) if result_dir is None else result_dir\n",
    "    outputs_dir = os.path.join(result_dir, 'outputs', 'inference')\n",
    "    var_dir = os.path.join(result_dir, 'outputs', 'var')\n",
    "    os.makedirs(outputs_dir, exist_ok=True)\n",
    "    os.makedirs(var_dir, exist_ok=True)\n",
    "\n",
    "    model.eval()\n",
    "    samples = dataset.samples\n",
    "    for i in range(len(samples)):  \n",
    "        for j in range(0,dataset.interval+1):\n",
    "            frame_idx = samples[i] + j\n",
    "            val_data_inputs, batchIndexGenerator =dataset._get_testing_data(frame_idx)\n",
    "            v = []\n",
    "            d = []\n",
    "            loop = tqdm.tqdm(batchIndexGenerator)\n",
    "            for current_idx, next_idx in loop:\n",
    "                coord = val_data_inputs[current_idx:next_idx]\n",
    "                with torch.no_grad():\n",
    "                    dat, var, out1, out2, out3 = model(coord)\n",
    "                    dat = dat.view(-1)\n",
    "                    var = var.view(-1)\n",
    "                    d.append(dat)\n",
    "                    v.append(var)\n",
    "            d = torch.cat(d,dim=-1).float()\n",
    "            d = d.detach().cpu().numpy()\n",
    "            d = np.asarray(d,dtype='<f')\n",
    "            out_path = f'{outputs_dir}/{frame_idx:04}.dat'\n",
    "            d.tofile(out_path, format='<f')\n",
    "            v = torch.cat(v,dim=-1).float()\n",
    "            v = v.detach().cpu().numpy()\n",
    "            v = np.asarray(v,dtype='<f')\n",
    "            var_path = f'{var_dir}/{frame_idx:04}.dat'\n",
    "            v.tofile(var_path, format='<f')\n",
    "    \n",
    "def adjust_lr(args, optimizer, epoch):\n",
    "    if args.lr_s=='exp':\n",
    "        lr = args.lr * math.exp(-0.02 * epoch)\n",
    "    elif args.lr_s=='step':\n",
    "        lr = args.lr * (0.5 ** (epoch // 50))\n",
    "    elif args.lr_s == 'cosine':\n",
    "        T_max = args.num_epochs\n",
    "        eta_min = 0\n",
    "        lr = eta_min + (args.lr - eta_min) * (1 + math.cos(math.pi * epoch / T_max)) / 2\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd92481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "p = argparse.ArgumentParser()\n",
    "p.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "p.add_argument('--gpu', type=str,default='0')\n",
    "p.add_argument('--seed', type=int, default=42)\n",
    "p.add_argument('--fp16', action=\"store_true\")\n",
    "p.add_argument('--compile', action=\"store_true\")\n",
    "# General training options\n",
    "p.add_argument('--downsample_factor', type=int, default=4, metavar='N',\n",
    "                    help='downsample factor')\n",
    "p.add_argument('--batch_size', type=int, default=8000)\n",
    "p.add_argument('--lr', type=float, default=1e-5, help='learning rate. default=1e-4')\n",
    "p.add_argument('--num_epochs', type=int, default=200,\n",
    "               help='Number of epochs to train for.')\n",
    "p.add_argument('--checkpoint', type=int, default=50,\n",
    "               help='checkpoint is saved.')\n",
    "p.add_argument('--ckpt', type=str,default=\"/MDSRN/checkpoints/-0-64-200.pth\",\n",
    "               help='checkpoint path.')\n",
    "p.add_argument('--dataset', type=str, default='fivejets',\n",
    "               help='Scalar dataset; one of (Vortex, combustion)')\n",
    "p.add_argument('--result_dir', type=str, default='./', metavar='N',\n",
    "                    help='the path where we stored the synthesized data')\n",
    "p.add_argument('--interval', type=int, default=0, metavar='N',\n",
    "                    help='temporal upscaling factor')\n",
    "p.add_argument('--active', type=str, default='sine', metavar='N',\n",
    "                    help='active function')\n",
    "p.add_argument('--init', type=int, default=64, metavar='N',\n",
    "                    help='init features')\n",
    "p.add_argument('--num_res', type=int, default=10, metavar='N',\n",
    "                    help='number of residual blocks')\n",
    "p.add_argument('--lr_s', type=str, default='cosine', help='step or exp')\n",
    "p.add_argument('--mode', type=str, default='inf', metavar='N',\n",
    "                    help='the path where we stored the synthesized data')\n",
    "opt = p.parse_known_args()[0]\n",
    "\n",
    "import torch\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = opt.gpu\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]  =  \"TRUE\"\n",
    "\n",
    "opt.cuda = not opt.no_cuda and torch.cuda.is_available()\n",
    "seed_everything(opt.seed)\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "def main():\n",
    "    print('FP16 enbled: ', opt.fp16)\n",
    "    print('Compile enbled: ', opt.compile)\n",
    "    Data = ScalarDataSet(opt)\n",
    "    Model = CoordNet(4,1, init_features=64, num_res=7)\n",
    "#    Model = CoordNetBottleNeck(4,1, init_features=288, num_res=1)\n",
    "#     Model = Nerf()\n",
    "    if opt.mode in ['inf', 'ue']:\n",
    "        ckpt = './'+opt.dataset+opt.ckpt\n",
    "        Model.load_state_dict(torch.load(ckpt))\n",
    "    if opt.compile:\n",
    "        Model.compile()\n",
    "    Model.cuda()\n",
    "\n",
    "    if opt.mode == 'train':\n",
    "        print('Initalize Model Successfully using Sine Function!')\n",
    "        trainNet(Model,opt,Data)\n",
    "    elif opt.mode == 'inf':\n",
    "        inf(Model, Data,opt)\n",
    "    \n",
    "if __name__== \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c5da87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#得到误差\n",
    "import os\n",
    "import numpy as np\n",
    "data_name = \"combustion\"#opt.dataset#'h2'\n",
    "origin_dir = './dataset/' + data_name + '/'\n",
    "recons_dir = './' + data_name + '/MDSRN/outputs/inference/'\n",
    "error_dir = './' + data_name + '/MDSRN/outputs/error/'\n",
    "var_dir = './' + data_name + '/MDSRN/outputs/var/'\n",
    "os.makedirs(error_dir, exist_ok=True)\n",
    "\n",
    "for i in range(1,2):\n",
    "    d = np.fromfile(recons_dir + '{:04d}.dat'.format(i), dtype='<f')\n",
    "    real = np.fromfile(origin_dir + '{:04d}.raw'.format(i), dtype='<f')\n",
    "    real = 2*(real-np.min(real))/(np.max(real)-np.min(real))-1\n",
    "    error = (real - d) ** 2    \n",
    "    error.tofile(error_dir+'{:04d}.dat'.format(i), format='<f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee48f858",
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算PSNR,norm\n",
    "import numpy as np\n",
    "import torch\n",
    "psnr = 0\n",
    "k=0\n",
    "psnr_fn_paper = lambda gt, pred, diff: 10. * torch.log10(diff**2 / torch.mean((gt-pred)**2))\n",
    "for i in range(1,2):\n",
    "    gt = np.fromfile(origin_dir + '{:04d}.raw'.format(i),dtype=np.float32)\n",
    "    d = np.fromfile(recons_dir + \"{:04d}.dat\".format(i),dtype=np.float32)\n",
    "    gt = 2*(gt-np.min(gt))/(np.max(gt)-np.min(gt))-1\n",
    "    d = torch.from_numpy(d)\n",
    "    gt = torch.from_numpy(gt)    \n",
    "    diff = gt.max() - gt.min()\n",
    "    \n",
    "    psnr_volume = psnr_fn_paper(gt, d, diff)\n",
    "    print(str(i)+\":\"+str(psnr_volume.item()))\n",
    "    psnr+=psnr_volume.item()\n",
    "    k+=1\n",
    "# print(psnr/k)\n",
    "def compute_nll(variance, error, epsilon=1e-8):\n",
    "    safe_variance = np.maximum(variance,  epsilon)\n",
    "    nll_terms = 0.5 * ( (error**2) / safe_variance + np.log(safe_variance) )\n",
    "    return np.mean(nll_terms)\n",
    "\n",
    "#计算corr,norm \n",
    "k = 0 \n",
    "t_corr = 0 \n",
    "for i in range(1,2): \n",
    "    k += 1 \n",
    "    v = np.fromfile(var_dir + \"{:04d}.dat\".format(i), dtype='<f') \n",
    "    e = np.fromfile(error_dir + \"{:04d}.dat\".format(i), dtype='<f') \n",
    "    corr = np.corrcoef(v, e)  \n",
    "    print(str(i)+\":\"+str(corr[0,1])) \n",
    "    t_corr+=corr[0,1] \n",
    "    nll = compute_nll(v, e)\n",
    "    print(str(i)+\":\"+str(nll))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
